# 08.用LLM生成英文学习内容

本章节将基于LangChain调用LLM生成英文学习内容


### 1. 什么是 LangChain？

**LangChain 是一个用于开发由大型语言模型（LLM）驱动的应用程序的开源框架**。它不仅提供了接口调用语言模型的能力，还可以将模型与数据源、记忆、工具、检索系统等组合成更复杂的智能工作流。
LangChain 的目标是简化 LLM 应用全生命周期：

* **连接数据和计算资源**，让模型不仅生成文本，还能处理外部信息。
* **构建链式逻辑**，将多个步骤组合起来实现复杂任务。
* **支持 Agent** 设计，使模型能够自动决定调用哪些工具和步骤。

它提供一套模块化的组件，可以用于构建从简单对话机器人到复杂检索增强生成（RAG）系统等各种应用。


### 2. LangChain 的作用与价值

LangChain 是构建 LLM 驱动应用的实用框架：

* **统一 LLM 接口**，简化模型调用
* **Prompt 模板与链结构**，提升复用和可维护性
* **支持对话管理、检索增强、Agents 架构**
* **模块化，能扩展到复杂工作流**

它提供了一个**成熟的抽象层**，让你像搭积木一样把 LLM 能力、提示模板、链逻辑、检索、工具调用等组合起来，构建强大的智能体和应用。

### 3. 基础使用指南

#### 第一步：安装依赖


```bash
pip install langchain langchain-openai
```


#### 第二步：学习使用PromptTemplate


在使用大语言模型（LLM）进行生成任务时，**提示词（Prompt）**是让模型理解任务、上下文和预期输出的关键信号。LangChain 提供了专门的类来帮助构建可复用、动态的提示模板，这就是 **PromptTemplate**。其作用是 **把用户输入与预定义格式组合成最终传给模型的提示语**。


**PromptTemplate 是一种模板工具**，它允许你将任务指令写成含变量的字符串模板，然后通过传入具体参数动态生成最终提示语。例如：

* **模板（带占位符）**：
  `"Tell me a {adjective} joke about {content}."`
* **生成提示**：
  `"Tell me a funny joke about chickens."`

在 LangChain 中，PromptTemplate 会根据你定义的变量（如 `adjective`、`content`）自动格式化提示语。


#####  如何创建 PromptTemplate

**方法 1：从模板字符串创建**

```python
from langchain_core.prompts import PromptTemplate

template = "Tell me a {adjective} joke about {content}."
prompt = PromptTemplate.from_template(template)
text = prompt.format(adjective="funny", content="chickens")
print(text)
```

这个例子中，`{adjective}` 和 `{content}` 是两个变量，它们会被具体参数替换生成完整提示。

**方法 2：指定输入变量**

你也可以显式指定变量列表：

```python
prompt = PromptTemplate(
    input_variables=["topic", "level"],
    template="Write a {level} level English paragraph about {topic}."
)
prompt.format(topic="AI ethics", level="intermediate")
```

这种方式适合变量较多或更复杂场景。

##### PromptTemplate 的优势

- **结构清晰**：把提示的固定部分与可变部分分离
- **复用性强**：同一模板可用于不同任务
- **可组合**：可以与链（Chains）、Agents 组合成复杂工作流
- **支持多种格式**：默认用 Python f-string，也可用 `jinja2` 进行动态渲染（注意安全性）


##### 与聊天模型结合：ChatPromptTemplate

当你要发送聊天式提示（有角色区分）时，LangChain 提供了 **ChatPromptTemplate**：

```python
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate
)

system = SystemMessagePromptTemplate.from_template(
    "You are an English tutor who explains grammar clearly."
)
human = HumanMessagePromptTemplate.from_template(
    "Explain this sentence: {sentence}"
)

chat_prompt = ChatPromptTemplate.from_messages([system, human])
chat_prompt.format_prompt(sentence="She had gone before I arrived.").to_messages()

这样可以构造结构化对话提示，其中包含 system、user 等多个角色消息。
```

#### 第三步：学习使用LangChain Expression Language（LCEL）


随着应用复杂度增加，**仅靠手写代码构建链（Chain）** 并不方便。LangChain 提供了 **表达式语言 LCEL**，它能用**声明式方式组合组件**，并可自动优化执行（包括同步、异步、流式等）。 

**什么是 LCEL？**

LCEL 是一种 **声明式语言**，让你不用写类结构代码，而用管道符 `|` 将组件串联起来。例如：

```
prompt | model | output_parser
```

这就相当于：

```python
chain = LLMChain(llm=model, prompt=prompt)
```

但 LCEL 的优势在于：

* 支持**同步/异步/流式**调用
* 可以访问**中间结果**
* 自动优化并行执行
* 更方便组合多个步骤（如检索 + 生成） 

下面使用 LCEL 将提示语和模型组合成一个链式流程：

```python
from langchain_core.prompts import PromptTemplate
from langchain_openai.chat_models import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

prompt = PromptTemplate.from_template(
    "Write an English paragraph about {topic} and list 3 vocabulary words."
)
model = ChatOpenAI(model="gpt-4o-mini")
output_parser = StrOutputParser()

# 使用 LCEL 表达式将 prompt、model、parser 串联起来
chain = prompt | model | output_parser

result = chain.invoke({"topic": "climate change"})
print(result)
```

这段代码做了以下几件事：

1. **PromptTemplate** 构建提示
2. **Model** 接受提示并生成内容
3. **OutputParser** 将模型输出转为字符串
4. **管道符 `|`** 自动连接上述步骤，从输入到生成再到解析完成整个链条

LCEL 的流程实际上等价于：

```
输入 dict → prompt.invoke → model.invoke → parser.invoke → 最终输出
```

至此，已经完成了 LangChain 的简单入门，更复杂的使用将在后续章节中陆续学习。