# 12.å¤šæ¨¡æ€

åœ¨è‹±è¯­å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œå£è¯­å’Œå¬åŠ›ä¹Ÿæ˜¯é‡è¦çš„ä¸€éƒ¨åˆ†ï¼Œå¦åˆ™å°±æˆäº†å“‘å·´è‹±è¯­ã€‚ä¸ºäº† è´´è¿‘çœŸå®è¯­è¨€ä½¿ç”¨åœºæ™¯ï¼Œæœ¬ç« èŠ‚å°†è®©é‚£ä¸ªè‹±è¯­å­¦ä¹  Agent æ”¯æŒã€Œè¯­éŸ³è¾“å…¥ + è¯­éŸ³è¾“å‡ºã€çš„å¤šæ¨¡æ€èƒ½åŠ›


## ä¸€ã€æ¶æ„æµç¨‹è°ƒæ•´

æ•´ä¸ªæµç¨‹å°†è°ƒæ•´å¦‚ä¸‹ï¼š

```
ğŸ™ï¸ User speaks
   â†“
ğŸ§© Speech-to-Text (ASR)
   â†“
ğŸ“œ Chat History + Prompt
   â†“
ğŸ§  LLM (English Tutor Agent)
   â†“
ğŸ“„ Text Response
   â†“
ğŸ”Š Text-to-Speech (TTS)
   â†“
ğŸ§ Audio Playback
```

## äºŒã€å‰ç«¯è°ƒæ•´

åœ¨Day09ä¸­è®²åˆ°Gradioæ”¯æŒInterface (å¿«é€Ÿæ¨¡å¼)å’ŒBlocks (è‡ªå®šä¹‰æ¨¡å¼)ï¼Œå‰é¢ç”¨çš„æ˜¯å¿«é€Ÿæ¨¡å¼ï¼Œç°åœ¨å·²ç»ä¸å¤Ÿç”¨äº†ã€‚è¿™é‡Œè¦åˆ‡æ¢ä¸ºè‡ªå®šä¹‰æ¨¡å¼ã€‚

1. Gradio ä¸­çš„è¯­éŸ³è¾“å…¥

Gradio åŸç”Ÿæ”¯æŒéº¦å…‹é£è¾“å…¥ï¼š

```python
gr.Audio(
    sources=["microphone"],
    type="filepath",
    label="Speak English"
)
```

è¿™é‡Œæ‹¿åˆ°çš„æ˜¯ï¼š

```text
/tmp/gradio/xxx.wav
```

2. ASR ç¤ºä¾‹

åŸºäºopenaiçš„whisper

```python
import whisper
asr_model = whisper.load_model("turbo")

def speech_to_text(audio_path: str) -> str:
    """
    æŠŠç”¨æˆ·è¯­éŸ³è½¬æˆæ–‡æœ¬
    """
    text = asr_model.transcribe(audio_path)
    return text
```

3. æŠŠè¯­éŸ³è¾“å…¥æ¥å…¥ç°æœ‰å¯¹è¯ Agent

ä¿®æ”¹åŸæ¥çš„è·å–å›å¤éƒ¨åˆ†ä»£ç ï¼Œå°†ä»¥ä¸‹ä»£ç ï¼š

```python
get_ai_response(user_message: str, history)
```

æ”¹æˆï¼š

```python
def voice_chat(audio_path, history):
    user_text = speech_to_text(audio_path)
    return get_ai_response(user_text, history)
```


4. è¯­éŸ³è¾“å‡ºï¼šText-to-Speechï¼ˆTTSï¼‰

```python

import edge_tts

def text_to_speech(text: str) -> str:
    """
    è¾“å…¥æ–‡æœ¬ â†’ è¾“å‡ºéŸ³é¢‘æ–‡ä»¶è·¯å¾„
    """
    audio_path = "./output.mp3"
    communicate = edge_tts.Communicate(text, "en-GB-SoniaNeural")
    with open(audio_path, "wb") as file:
        for chunk in communicate.stream_sync():
            if chunk["type"] == "audio":
                file.write(chunk["data"])
    return audio_path
```

5. æŠŠ Agent å›å¤è½¬æˆè¯­éŸ³
åŸºäºtts-edgeå®ç°

```python
response_text = get_ai_response(user_text, history)
audio_path = text_to_speech(response_text)

return response_text, audio_path
```

6. è°ƒæ•´UI

åŸºäºGradioçš„è‡ªå®šä¹‰æ¨¡å¼å®ç°ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ§ è‹±è¯­å£è¯­å­¦ä¹ åŠ©æ‰‹")

    audio_input = gr.Audio(
        sources=["microphone"],
        type="filepath",
        label="Speak English"
    )

    text_output = gr.Chatbot()
    audio_output = gr.Audio(label="AI Voice Reply")

    audio_input.change(
        fn=voice_chat,
        inputs=[audio_input, text_output],
        outputs=[text_output, audio_output]
    )
```

è‡³æ­¤ï¼Œè‹±è¯­å­¦ä¹ Agentæ”¯æŒè¯­éŸ³è¾“å…¥è¾“å‡ºäº†ã€‚