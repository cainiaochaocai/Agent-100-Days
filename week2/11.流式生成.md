# 11.流式生成

在前面学习LLM时已经了解到LLM的本质是预测下一个Token，因此LLM的输出一个Token一个Token的输出。
但当前的对话模式是*用户输入 → 等待 → 一次性返回*。这会导致几个问题。

1. **响应感知慢**，用户只能等全部生成后才能得到结果
2. 返回过程中，**无法中断**,用户只能等

本章节的目的就是支持流式返回输出。这样可以更接近模型真实运行方式，并显著改善用户体验。这也是「思考可视化」「中断控制」的前置条件

## 一、流式 vs 非流式

当前代码是调用LLM的invoke方法

```python
response = llm.invoke(prompt)
print(response.content)
```

这种方式的特点是：程序会一直等待，直到模型全部生成完成后，才返回结果,中间过程完全不可见。要想流式调用，需要使用stream方式，代码如下：

```python
for chunk in llm.stream(prompt):
    print(chunk.content, end="")
```
在这种调用方式下，模型生成一点，就立刻返回一点，这让用户产生一种非常自然的感觉。

## 二、简单的流式示例


当执行以代码后，可以看到流式生成的效果。

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="qwen-flash", base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")

for chunk in llm.stream("解释什么是 Agent"):
    print(chunk.content, end="", flush=True)
```

但也会注意到在一点一点“打”出来的过程中，有时候会一连好几个字，有时候也会卡一下（但很短暂）。这是模型和网络传输的正常行为。一个chunk并不等于一个字或一个单词。
其中对应空chunk的情况需要特别注意，空chunk有可能是模型在“思考”和“切换状态”，也有可能是网络在分段传输数据，这种情况需要在开发过程中处理好。


## 三、让Agent支持流式输出


### 1. 让LLM支持流式

让LLM支持流式很简单，只需要将参数stream设置为True就可以了，代码如下：

```python
llm = ChatOpenAI(model="qwen-flash", 
                 base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
                 streaming=True)

```

### 2.让UI支持流式

在Gradio中是通过generator的机制实现流式，即yield方法，代码如下：

```python
def respond():
    yield "部分内容"
```

### 3. 在调用时使用流式

这里需要注意的是，

```python
def respond(message, history):
    history = history or []
    prompt_input = {
        "history": history,
        "input": message
    }

    partial_reply = ""

    for chunk in chain.stream(prompt_input):
        if chunk.content:
            partial_reply += chunk.content
            yield history + [(message, partial_reply)]
```

---

## 六、完整 Streaming 对话 Agent 示例

```python
import gradio as gr
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

llm = ChatOpenAI(
    model="gpt-4o-mini",
    streaming=True
)

prompt = ChatPromptTemplate.from_messages([
    ("system", "你是一个清晰、有条理的助手"),
    ("placeholder", "{history}"),
    ("user", "{input}")
])

chain = prompt | llm

def respond(message, history):
    history = history or []
    partial = ""

    for chunk in chain.stream({
        "history": history,
        "input": message
    }):
        if chunk.content:
            partial += chunk.content
            yield history + [(message, partial)]

with gr.Blocks() as demo:
    chatbot = gr.Chatbot()
    msg = gr.Textbox(label="输入")
    msg.submit(respond, [msg, chatbot], chatbot)

demo.launch()
```

至此，一个支持流式输出的Agent就完成了

> 附：SSE介绍

