# 11.流式生成

在前面学习LLM时已经了解到LLM的本质是预测下一个Token，因此LLM的输出一个Token一个Token的输出。
但当前的对话模式是*用户输入 → 等待 → 一次性返回*。这会导致几个问题。

1. **响应感知慢**，用户只能等全部生成后才能得到结果
2. 返回过程中，**无法中断**,用户只能等

本章节的目的就是支持流式返回输出。这样可以更接近模型真实运行方式，并显著改善用户体验。这也是「思考可视化」「中断控制」的前置条件

## 一、流式 vs 非流式

当前代码是调用LLM的invoke方法

```python
response = llm.invoke(prompt)
print(response.content)
```

这种方式的特点是：程序会一直等待，直到模型全部生成完成后，才返回结果,中间过程完全不可见。要想流式调用，需要使用stream方式，代码如下：

```python
for chunk in llm.stream(prompt):
    print(chunk.content, end="")
```
在这种调用方式下，模型生成一点，就立刻返回一点，这让用户产生一种非常自然的感觉。

## 二、简单的流式示例


当执行以代码后，可以看到流式生成的效果。

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="qwen-flash", base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")

for chunk in llm.stream("解释什么是 Agent"):
    print(chunk.content, end="", flush=True)
```

但也会注意到在一点一点“打”出来的过程中，有时候会一连好几个字，有时候也会卡一下（但很短暂）。这是模型和网络传输的正常行为。一个chunk并不等于一个字或一个单词。
其中对应空chunk的情况需要特别注意，空chunk有可能是模型在“思考”和“切换状态”，也有可能是网络在分段传输数据，这种情况需要在开发过程中处理好。


## 三、让Agent支持流式输出


### 1. 让LLM支持流式

让LLM支持流式很简单，只需要将参数stream设置为True就可以了，代码如下：

```python
llm = ChatOpenAI(model="qwen-flash", 
                 base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
                 streaming=True)

```

### 2.让UI支持流式

在Gradio中是通过generator的机制实现流式，即yield方法，代码如下：

```python
def respond():
    yield "部分内容"
```

### 3. 在调用时使用流式

这里需要注意的是，

```python
def respond(message, history):
    history = history or []
    prompt_input = {
        "history": history,
        "input": message
    }

    partial_reply = ""

    for chunk in chain.stream(prompt_input):
        if chunk.content:
            partial_reply += chunk.content
            yield history + [(message, partial_reply)]
```

---

## 六、完整 Streaming 对话 Agent 示例

```python
import gradio as gr
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

llm = ChatOpenAI(
    model="gpt-4o-mini",
    streaming=True
)

prompt = ChatPromptTemplate.from_messages([
    ("system", "你是一个清晰、有条理的助手"),
    ("placeholder", "{history}"),
    ("user", "{input}")
])

chain = prompt | llm

def respond(message, history):
    history = history or []
    partial = ""

    for chunk in chain.stream({
        "history": history,
        "input": message
    }):
        if chunk.content:
            partial += chunk.content
            yield history + [(message, partial)]

with gr.Blocks() as demo:
    chatbot = gr.Chatbot()
    msg = gr.Textbox(label="输入")
    msg.submit(respond, [msg, chatbot], chatbot)

demo.launch()
```

至此，一个支持流式输出的Agent就完成了

> 附：SSE介绍
> 在大多数 Web 应用中，LLM 的流式输出**最终都会落到一种机制上：SSE（Server-Sent Events）**。
>
> #### 1、为什么需要 SSE？
>
> **传统请求的问题**
>
> 在非流式模式下：
>
> ```text
> 浏览器 ── 请求 ──> 服务端
> 浏览器 <─ 完整响应 ─ 服务端（等 10 秒）
> ```
>
> 问题是：
>
> * 用户 **长时间看不到任何反馈**
> * 无法中途取消
> * 无法展示“思考过程”
>
> **流式请求的目标**
>
> 我们希望的是：
>
> ```text
> 浏览器 ── 请求 ──> 服务端
> 浏览器 <─ token 1
> 浏览器 <─ token 2
> 浏览器 <─ token 3
> ...
> ```
>
> 这正是 **SSE 解决的问题**。
>
>
> #### 2、什么是 SSE（Server-Sent Events）？
>
> **SSE 是一种服务端 → 浏览器 单向推送数据的 HTTP 流式机制**
>
> 特点：
>
> * 基于 **HTTP（不是 WebSocket）**
> * 长连接
> * 服务端可以不断 `push` 消息
> * 浏览器原生支持（`EventSource`）
>
>
> #### 3、SSE 的数据长什么样？
>
> SSE 的数据是**纯文本协议**，每条消息长这样：
>
> ```text
> data: Hello
> ```
>
> 多条连续发送：
>
> ```text
> data: Hello
> data: world
> data: !
> ```
>
> 浏览器会**一条一条接收并触发事件**。
>
>
> ### 四、LLM Streaming 与 SSE 的关系
>
> 在 LLM 场景中：
>
> ```text
> 模型生成 token
>     ↓
> 后端不断 flush 响应
>     ↓
> 通过 SSE 推送到前端
>     ↓
> 前端实时更新 UI
> ```
