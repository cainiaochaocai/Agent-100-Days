# 思维链

在这一章中将深入探讨 AI 提示工程中最为关键的进阶技术：**思维链 (Chain of Thought, CoT)**。

## 一、单 Prompt 的天然局限

在实际的业务功能实现中，随着处理的任务越来越复杂，往往会导致提示词越来越长，约束越来越精细，但结果还是不稳定。因为单Prompt天生就不稳定。原因如下：

#### 1.隐式推理不可控

模型一定会“想”，但它在**内部想**，你看不到、也干预不了。

结果就是：同一个问题，多次输出结果不同。如果错了，你不知道错在“哪一步”，而且很难插入检查、纠偏、工具调用等。

**示例**

> “帮我判断这个创业项目值不值得投。”

模型可能直接给你一个“值得 / 不值得”，
但你不知道它依据的是市场？产品？团队？情绪？

#### 2.长 Prompt难以维护

很多人解决复杂任务的方式是：**不断往一个 Prompt 里堆规则。**

结果通常是指令互相干扰、模型抓不住真正目标，导致效果越来越差。

**示例**

一个投研 Prompt 同时要求：

* 给结论
* 做财务分析
* 控制字数
* 模拟风格
* 输出表格
* 风险提示
* 给评级

结果模型开始“抽风式满足”。

#### 3.无法表达“过程结构”

真实复杂任务最好是分阶段、有中间状态、可回退、可验证，而单 Prompt 是**无状态黑盒调用**。

## 二、思维链的基本思想

**思维链 (Chain of Thought, CoT)** 的核心非常简单：**强制模型输出“思考过程”**。通过在 Prompt 中加入“**Let's think step by step**”（让我们一步步思考）或者提供带推理步骤的示例（Few-shot CoT），模型会被引导先生成推理路径，再得出最终结论。

从“直接给我结果”变成“一步一步思考，再给我结果”。

**示例**

普通 Prompt：

> 一个商品原价 80，先涨价 25%，再打 8 折，现在多少钱？

模型经常直接给一个数，错了也不知道为什么。

思维链Prompt：

> 请一步一步计算价格变化过程，再给最终结果。

输出变成：

* 第一步：80 × 1.25 = 100
* 第二步：100 × 0.8 = 80
* 最终价格：80

这样正确率显著提升，思维链的效果不是“更聪明”，而是**被迫外显推理路径**。

这样产生的真正价值不在“多想几步”， 而是推理可见、中间状态可利用、错误可定位、流程可编排。

**与推理模型（如 o1, DeepSeek-R1）的关联**

推理模型（Reasoning Models）本质上就是在训练阶段，被系统性奖励“正确推理路径”的模型。所以推理模型在底层就集成了这种能力。它们不再是“说出”思考过程，而是在给出答案前，先在后台进行大量的隐式强化学习推理。

因此，使用普通模型需要你强制写 “一步一步思考”，而在使用推理模型时，它会会自动拆步骤。

## 三、CoT vs 一次性长 Prompt

很多人会把“写一段很长的指令”误认为思维链。其实两者有本质区别：

| 特性       | 一次性长 Prompt (Long Prompt) | 思维链 (CoT)       |
| -------- | ------------------------- | --------------- |
| **重点**   | 强调约束、背景、格式要求              | 强调**逻辑演化**和中间步骤 |
| **执行逻辑** | “按要求做”                    | “先想逻辑，再做决定”     |
| **容错率**  | 较低（一步错全盘错）                | 较高（可以通过中间步骤纠错）  |

### 四、思维链的延伸

当 CoT 结合了外部干预和流程控制，它就演变成了 **Agent (智能体)** 的雏形：

1. **Prompt Chaining (提示词链)**：将一个复杂的 CoT 拆解成多个隔离的步骤。将 Prompt A 的思考结果提取出来，传给 Prompt B。由于每个环节只处理一小块逻辑，模型对该环节的专注度极高，有效避免了长 Prompt 的指令干扰。
2. **ReAct 模式**：模型先思考 (Reason)，发现缺数据，于是去行动 (Act) 查搜索，观察结果后 (Observe) 再继续思考。这是让 AI 学会使用工具（搜索、计算器、API）的核心模式。
3. **Self-Correction (自我修正)**：在思维链中增加一步：“检查上述步骤是否有逻辑漏洞，如有请修正”。这就像学生做完卷子后的“检查一遍”。这种自我博弈（Self-Reflection）能显著降低模型产生“幻觉”的概率。