# 24.基于 Langchain 的工具调用

在理解了工具调用的机制后，我们需要通过代码将其落地。目前最流行的框架是 **LangChain**，它将复杂的 JSON Schema 封装成了简单的装饰器和对象。

以下是基于 LangChain 实现工具调用示例。

## 一、LangChain 在工具调用里做了什么？

LangChain 本身**不会增强模型能力**，它主要帮你做了三件事：

1. 把函数包装成标准 Tool
2. 把 Tool Schema 注入给模型
3. 把模型的结构化输出，自动路由到真实函数执行

## 二、一个最小可用示例：天气查询

### 1. 定义真实工具函数

```python
def get_weather(city: str) -> str:
    # 这里正常应该调用真实 API
    fake_weather_db = {
        "上海": "小雨，湿度 90%",
        "北京": "晴，温度 10°C"
    }
    return fake_weather_db.get(city, "未查询到该城市天气")
```

### 2. 用 LangChain 把函数注册成 Tool

```python
from langchain.tools import tool

@tool
def get_weather(city: str) -> str:
    """当用户询问实时天气情况时，使用该工具获取指定城市的天气信息。"""
    fake_weather_db = {
        "上海": "小雨，湿度 90%",
        "北京": "晴，温度 10°C"
    }
    return fake_weather_db.get(city, "未查询到该城市天气")
```

这里 LangChain提供了`@tool`装饰器，它自动根据函数的**名称**、**注释和参数类型**生成我们上一节讲到的Tool Schema

> 注意函数下方的字符串（Docstring）。这是 **Description** 的唯一来源，如果你不写或者写得模糊，模型将不知道何时调用它。

### 3. 绑定工具

定义好工具后，我们需要将其“挂载”到大模型上。不同的模型（OpenAI, Claude, 阿里云百炼）底层协议不同，但 LangChain 提供了统一的接口 `.bind_tools()`。

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="qwen-max", 
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")

tools = [get_weather]
tool_map = {t.name: t for t in tools}
llm_with_tools = llm.bind_tools(tools)
```

此时的 `llm_with_tools` 已经是一个“具备工具能力”的模型实例了。

### 4. 调用模型

```python
from langchain_core.messages import HumanMessage

response = llm_with_tools.invoke([
    HumanMessage(content="帮我查一下今天上海的天气，顺便判断适不适合洗车")
])

print(response)
```

此时模型**大概率不会给自然语言回答**，而是返回类似：

```python
AIMessage(
    content="",
    tool_calls=[{
        "name": "get_weather",
        "args": {"city": "上海"},
        "type": "tool_call"
    }]
)
```

### 5. 执行工具

这是最容易出错的地方。模型输出 `tool_calls` 后，**模型并不会自动运行函数**，你需要手动或通过框架运行，这里以手动执行为例。

```python
tool_messages = []

for call in response.tool_calls:
    tool_name = call["name"]
    tool_args = call["args"]

    tool = tool_map[tool_name]
    result = tool.invoke(tool_args)

    tool_messages.append(
        ToolMessage(
            content=str(result),
            tool_call_id=call["id"]
        )
    )

print(tool_messages)
```

得到：

> 小雨，湿度 90%

### 6. 把结果回传给模型，继续推理

```python
final_response = llm_with_tools.invoke([
    HumanMessage(content="帮我查一下今天上海的天气，顺便判断适不适合洗车"),
    response,
    *tool_messages
])

print(final_response.content)
```

输出：

> 今天上海的天气是小雨，湿度达到了90%，这种天气条件下不太适合洗车，因为高湿度和降雨可能会使车辆表面难以干透，并且刚洗完可能又会被雨水弄脏。

至此，一条完整链路闭环：

```
用户 → 模型 → 工具意图 → 系统执行 → 真实结果 → 模型再推理 → 输出
```

## 三、LangChain 工具调用流程拆解

从系统视角看，一次完整的 LangChain 工具调用，本质上拆成四个阶段：

> **Schema 注入 → 调用意图生成 → 工具执行 → 结果回传与再推理**

### 1.Schema 注入

这一阶段发生在**模型调用之前**，由 LangChain 框架完成。

核心工作只有一件事, 即将 Python 函数或类转化为模型可理解的**语言标准**。

**转换链条：** `Python 函数`->`LangChain Tool`->`JSON Schema`->`模型 API 参数 (如 tools 字段)`。

### 2.工具调用意图生成

当用户输入进入模型后，模型输出的不再只是文本，而是一个结构化的 **AIMessage**，其中可能包含 `tool_calls` 字段，例如：

```json
{
  "name": "get_weather",
  "arguments": {
    "city": "上海"
  }
}
```

这一刻，模型完成的事情只有一件**表达“我想调用哪个工具，用什么参数”。**

非常关键的一点是模型**不会**执行工具,模型也**不知道**工具是否真的存在，模型只是输出了一段“调用提案”。即模型只负责“决策”。

### 3.实际工具调用

真正的函数执行，完全发生在模型之外，由 LangChain 的工具执行系统接管，例如：

```python
tool_message = get_weather.invoke(tool_call)
```

LangChain 会根据 `name` 做路由，找到真实函数并执行，然后构造 `ToolMessage` 返回给模型。

在这一层，LangChain 提供的是一个“工具执行基础设施”，而不仅是函数调用封装，核心能力包括：

#### a.路由与上下文封装

`ToolCallRequest` 会封装完整执行上下文,包括参数、来源消息、调用 id、运行配置等

#### b.中间件系统

工具执行的核心不是 invoke，而是**中间件机制**：

* 执行前拦截（参数校验、权限控制、限流）
* 执行中控制（超时、中断、并发调度）
* 执行后处理（结果改写、日志、审计、缓存）

中间件可以跳过真实执行、修改参数、包装异常、甚至完全接管执行流程。

#### c. 健壮性控制

Langchain的工具异步执行、并行调用、错误捕获与恢复等健壮性控制。

## 四）结果回传与再推理

工具执行完成后，LangChain 会把结果封装成 `ToolMessage`，重新传给模型。

模型在此基础上进行第二次推理，判断是继续调用新工具，还是信息整合生成最终答案。
