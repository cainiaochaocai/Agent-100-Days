# 大模型之后是什么？从《哥德尔、巴赫、艾舍尔》看 AI 的下一步

如果你最近在做 AI 产品，可能会深陷于一种**“认知分裂”**：

- **Demo 越来越惊艳**：发布会上能力列表越拉越长，多步推理、复杂编程信手拈来。
- **业务频繁“翻车”**：一旦进入真实业务场景，逻辑断层、幻觉频发，在看似简单的问题上表现得像个“智力断崖”的巨婴。

模型看起来无所不知，却又不像真的懂。

一个终极拷问开始被反复提起：**大语言模型，究竟是在逼近“智能”的本质，还是仅仅把“概率拟合”玩到了极致？**

重读神作《哥德尔、巴赫、艾舍尔》（GEB）后，我产生了一些启发，与各位交流。

## 一、 我们得到了“能力”，但没有得到“理解”

从工程角度看，大模型的成就毋庸置疑：语言生成、代码补全、流程编排。但当你真正将其产品化时，会发现一个核心断层：

> **模型很会“说正确的话”，但它并不知道“为什么是对的”。**

它更像一个反应极快、表达力极强的系统，而非一个真正理解世界的智能体。一个自然的工程假设是：**“再堆一点算力，模型大到一定程度，会不会就‘自然懂了’？”**

GEB 给出的启示是：**不一定，而且很可能不会。** 因为理解不仅仅是信息的累加，更是结构的涌现。

## 二、 杨立昆的批评：文本智能的“天花板”

杨立昆（Yann LeCun）曾多次直言：现有模型本质上是在处理文本的“表面统计”，它们从未真正理解物理世界的因果律。

这指向了认知科学中经典的**符号接地问题（Symbol Grounding Problem）**：

- **大模型的“重力”**：源于数万亿 Token 的统计相关性。它知道“重力”常和“下落”出现在同一个句子。
- **五岁孩子的“重力”**：源于无数次看见物体掉落，以及自己摔跤时的切肤之痛。

这两种“理解”的来源完全不同。

## 三、 关键不在模型，而在数据形态

如果把不同智能系统所依赖的“输入来源”放在一起对比，会发现一个非常本质的问题：**今天的大语言模型，几乎只活在“文本世界”里。**

它们接受了规模空前的语言训练——大约数十万亿级别的 Token，相当于一个人连续阅读几十万年。这让模型极其擅长社会语言、符号体系与显性知识：如何表达观点、如何组织逻辑、不同语境下人们通常会说什么。

但真正的智能，并不只来自“阅读”。

人类认知中还有两类更底层、却几乎缺席于当前 LLM 训练体系的输入来源。

第一类是**感官流**。一个四岁的孩子，在日常生活中通过视觉、听觉、触觉持续接收的原始信息量，早已达到天文级规模。正是这些连续不断的感官刺激，让大脑逐步形成对世界的直觉模型：物体会掉落、液体会流动、空间是连续的、遮挡会改变可见性。这些并不是通过“描述”学会的，而是通过长期感知逐步内化成“物理常识”。

第二类是**具身交互**。人类从婴儿时期开始，就在不断进行“行动 → 反馈 → 修正”的闭环：伸手、抓取、跌倒、调整、再尝试。在这个过程中，大脑不仅学习如何控制身体，更在建立最核心的能力——因果模型。什么行为会带来什么结果，什么改变是我导致的，什么改变来自外界。这种实时因果反馈，是推理能力真正的地基。

而大语言模型，几乎完全缺失这两类输入。

它们读过几乎所有能读到的文字，却从未真正“看见”过世界，也从未“动手”改变过世界。

这直接导致一个典型现象：
模型在语言与抽象符号层面极其聪明，却缺乏对现实世界的直觉约束。

因此，它可以写出极其优美的诗歌，可以总结复杂的哲学观点，却会在极其基础的物理常识上犯错，比如：把一个装满水的瓶子倒过来，水会发生什么变化。

文本训练赋予的是**表达能力与显性知识**，而不是**直觉物理与因果经验**。

从这个角度看，当下 LLM 的很多“幻觉”和“常识错误”，并不是参数规模不够，而是输入世界不完整。它们缺失的不是更多文本，而是那块最关键的补丁：**感知与行动。**

这也是为什么，多模态、具身智能、可交互 Agent，并不是“功能扩展”，而是在补齐智能系统最底层的认知地基。

## 四、 世界模型：从“语言连续”走向“因果连续”

这正是为什么“大模型之后”的共识，正在转向 **World Models（世界模型）**。

尤其是杨立昆提出的 **JEPA（联合嵌入预测架构）**。其核心思想是：**不再预测“下一个 Token”，而是预测“世界状态的演化”。**

这是从“语言智能”走向“环境智能”的关键一步。只有当模型能够预测“如果我这样做，世界会发生什么”时，它才开始拥有真正的理解。

## 五、 哥德尔不完备性：为什么系统需要“元认知”

GEB 的第一条主线来自哥德尔不完备性定理。将其工程化，可以得到一个结论：

> **任何足够复杂的系统，都存在它在系统内部无法证明的真理。**

映射到 AI 系统中：模型越强，覆盖的问题空间越大，它就越不可能仅靠内部概率来判断“自己是否在胡说”。

下一步的关键不是“更准”，而是**知道什么时候不确定**。这正是搜索增强、强化学习和推理校验（Reasoning）的真正价值——它们不是让模型更聪明，而是为系统引入了**元认知（Meta-cognition）**。

## 六、 神经符号智能：给“醉酒诗人”装上“刹车”

当前 AI 圈正在经历一场“神经符号复兴”。用 GEB 的隐喻来说：

- **LLM 像“醉酒诗人”**：联想丰富、生成迅速，但偶尔逻辑混乱。
- **未来的 AI 需要一位“严肃会计师”**：逻辑严谨、可校验、可解释。

这对应了丹尼尔·卡尼曼的框架：**系统 1（直觉/联想）与系统 2（逻辑/校验）**。今天的大模型是极致的系统 1，而神经符号方法本质上是在为 AI 引入系统 2。

## 七、 巴赫与艾舍尔：从“模式模仿”到“自我闭环”

巴赫的音乐告诉我们：创造力不来自规则的缺失，而来自**在极端约束下的结构构建**。

而艾舍尔的“怪圈”则揭示了“自我”的来源：当一个系统开始**描述自己、影响自己、并根据这种描述调整行为**时，真正的智能才会出现。

目前的大多数 AI 依然是“失忆”的：

- 没有稳定的自我边界。
- 没有连续的行为历史。
- 没有针对错误的自我修正机制。

## 八、 Agent

很多人会觉得：
Agent 不就是给 LLM 包一层流程吗？

但如果从 GEB 的视角看，Agent 的意义完全不同。

Agent 是第一次在工程层面，把三件事放在了一起：

1. **反思与不确定性**（哥德尔）
2. **结构化规划与组合**（巴赫）
3. **系统作用于自身**（艾舍尔）

这不是能力升级，而是**结构升级**。

## 结语

《哥德尔、巴赫、艾舍尔》帮我们看清了一件事：

**智能，从来不等于“可证明的正确性”。**

真正的智能系统，是明知自己不完备，却仍然能够行动；是在反馈中修正，在不确定中前进。大模型只是这趟长征的起点。**真正的挑战，是构建一个能“看见自己”的系统。