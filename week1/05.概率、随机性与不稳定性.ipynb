{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13d7e005-7683-4be8-a5cb-8eb330e4cf37",
   "metadata": {},
   "source": [
    "# 05.概率、随机性与不稳定性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21225650-f9a6-461a-bf7b-f8f50141c2af",
   "metadata": {},
   "source": [
    "前面已经讲到，LLM的本质是预测下一个Token，每次返回的是一个概率分布，所以一次回答是多次概率采样的连锁结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f314b-a898-4c1e-8d0e-c5a64bedee94",
   "metadata": {},
   "source": [
    "## 一、为什么同一个问题多次询问会得到不同答案"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98843d2-34e5-4198-b23b-f0373b688e2e",
   "metadata": {},
   "source": [
    "假设输入是：“天空是___”\n",
    "模型计算出的概率可能是：\n",
    "\n",
    "* **蓝色的**：0.7\n",
    "* **阴沉的**：0.15\n",
    "* **广阔的**：0.1\n",
    "* **红色的**：0.05\n",
    "\n",
    "如果模型每次都只选概率最高的那一个（贪婪搜索），那么它确实会永远给出相同的答案。但在实际应用中，为了让语言更自然、更有创造力，会引入**随机采样**。\n",
    "\n",
    "需要注意的是这里的“不同”并不等同于“错误”。在向量空间中，“正确答案”往往不是唯一的**，尤其是解释类、总结类、推理路径类的问题。LLM的目标不是“一致性”，而是**在统计意义上像人类写的文本**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2edaf55-2f99-4612-8022-01c2dd5522fa",
   "metadata": {},
   "source": [
    "### 实战：同一问题，多次采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed10730b-7fd3-4bb6-920b-202b12835d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API Key loaded\n"
     ]
    }
   ],
   "source": [
    "# 加载环境变量\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"请先配置 OPENAI_API_KEY\"\n",
    "print(\"✅ API Key loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d3ead0-29ed-4ca1-8aab-7f385451b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\")\n",
    "\n",
    "def ask_llm(prompt, temperature=0.7, top_p=1.0):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"qwen-flash\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "085c7ed4-621d-47cd-9e14-f2b108a2d65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 第 1 次 ---\n",
      "大语言模型是一种基于海量文本数据训练的深度学习模型，能够理解并生成人类语言。它通过学习语言中的统计规律，具备回答问题、撰写文章、翻译语句等自然语言处理能力。其“大”主要体现在参数量巨大，从而拥有更强的泛化和上下文理解能力。\n",
      "\n",
      "--- 第 2 次 ---\n",
      "大语言模型是一种基于大量文本数据训练的深度学习模型，能够理解、生成和推理自然语言。它通过学习语言中的统计规律，实现文本续写、问答、翻译等多种语言任务。因其庞大的参数量和强大的泛化能力，大语言模型在各类自然语言处理应用中表现出色。\n",
      "\n",
      "--- 第 3 次 ---\n",
      "大语言模型是一种基于海量文本数据训练的深度学习模型，能够理解并生成接近人类水平的自然语言。它通过预测下一个词或补全文本，在对话、写作、翻译等任务中表现出强大的语言能力。其核心优势在于“预训练+微调”的模式，能适应多种不同的应用场景。\n",
      "\n",
      "--- 第 4 次 ---\n",
      "大语言模型是一种基于海量文本训练的深度学习模型，能够理解并生成人类语言。它通过分析大量语料库中的语言模式，具备强大的文本生成、问答、翻译和摘要等能力。尽管其表现接近人类水平，但模型本身并无意识或理解力，仅依赖统计规律进行预测。\n",
      "\n",
      "--- 第 5 次 ---\n",
      "大语言模型是一种基于海量文本数据训练的深度学习模型，能够理解和生成人类语言。它通过捕捉语言中的统计规律，实现文本生成、问答、翻译等多种自然语言处理任务。其“大”主要体现在参数量巨大，具备强大的泛化能力和上下文理解能力。\n"
     ]
    }
   ],
   "source": [
    "prompt = \"请用三句话解释什么是大语言模型。\"\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- 第 {i+1} 次 ---\")\n",
    "    print(ask_llm(prompt, temperature=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13515fe3-3a7d-49db-b77d-14b57a512a8f",
   "metadata": {},
   "source": [
    "**结果分析**：从上述结果可以看到，虽然返回结果的核心意思一致，但每次的侧重点都不一样。\n",
    "\n",
    "既然模型输出的多样性并不是随机的“噪声”，而是由生成策略和控制参数决定的。那么，我们该如何控制这种随机性的“度”？\n",
    "\n",
    "答案是通过调整 temperature、top-k 或 top-p 等采样参数，我们可以控制模型在生成时选择高概率词的倾向，或者允许它尝试概率较低但更有创意的词汇。理解这些参数如何影响生成过程，是解释为什么同一个问题会产生不同回答的核心，也是学习如何平衡一致性与创造性的基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e489183-c6d7-4c45-970d-0939d2d465b3",
   "metadata": {},
   "source": [
    "## 二、temperature等参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4de9f06-c858-4eb6-8c35-ec453b8862fd",
   "metadata": {},
   "source": [
    "1. **Temperature（温度）**\n",
    "\n",
    "   * 控制概率分布的平滑程度。\n",
    "   * **低温度（如 0.1–0.3）**：模型更“保守”，高概率词更容易被选中 → 输出更稳定、一致性高。\n",
    "   * **高温度（如 0.7–1.0）**：概率分布更平坦，低概率词更有机会被采样 → 输出更丰富、创造性更强。\n",
    "\n",
    "2. **Top-k 采样**\n",
    "\n",
    "   * 仅从概率排名前 k 的候选词中进行采样。\n",
    "   * 限制范围可以避免模型选择极低概率的词，提高生成合理性。\n",
    "   * k 越小 → 输出越确定，k 越大 → 输出更多样化。\n",
    "\n",
    "3. **Top-p 采样（又称 nucleus 采样）**\n",
    "\n",
    "   * 选择累积概率达到 p 的候选词集合进行采样。\n",
    "   * 比 Top-k 更灵活，因为候选数量会根据概率分布自动调整。\n",
    "   * p 越小 → 输出越保守，p 越大 → 输出更自由、多样。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41261ae8-812f-49af-9fc4-5e3451be7d1c",
   "metadata": {},
   "source": [
    "其中Temperature和Top-k比较好理解，而Top-p有点复杂，这里补充说明下：\n",
    ">Nucleus Sampling 的工作原理：\n",
    ">1. 计算累积概率：\n",
    ">  - 首先，对模型输出的概率分布进行排序，并计算词汇的累积概率。\n",
    ">  - 然后，根据预先设定的阈值 p，确定一个最小集合，使得这个集合内的词汇的累积概率总和不超过 p。\n",
    ">2. 动态选择词汇：\n",
    ">  - 在每个时间步，根据计算出的累积概率和阈值 p，动态地选择词汇。\n",
    ">  - 如果累积概率中的某个词汇使得总概率超过 p，则剔除该词汇并重新分配剩余概率给其他词汇。\n",
    ">3. 控制多样性：\n",
    ">  - 通过调节阈值 p 的大小，可以控制生成文本的多样性。较小的 p 值会导致选择的词汇更加集中，而较大的 p 值会增加选择的词汇范围。\n",
    ">示例说明：\n",
    ">假设我们有一个词汇表，包含以下单词及其对应概率：\n",
    ">- A: 0.3\n",
    ">- B: 0.2\n",
    ">- C: 0.15\n",
    ">- D: 0.1\n",
    ">- E: 0.1\n",
    ">- F: 0.05\n",
    ">- G: 0.05\n",
    ">- H: 0.05\n",
    ">假设我们设置阈值 p=0.6。\n",
    ">1. 计算累积概率：\n",
    ">  - 累积概率序列为：0.3, 0.5, 0.65, 0.75, 0.85, 0.9, 0.95, 1.0。\n",
    ">2. 动态选择词汇：\n",
    ">  - 当累积概率超过 p=0.6 时停止选择，例如选择前三个单词 A, B, C。\n",
    ">  - 如果某单词的累积概率使得总和超过 p，则该单词会被剔除，例如单词 D。\n",
    ">通过 Nucleus Sampling，模型可以在保持一定多样性的同时，根据上下文动态地选择词汇，使生成文本更加丰富和有趣。通过调节阈值 p，可以控制生成文本的多样性和可控性，使其更>符合期望。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5fb42b-a3be-4b3b-955c-06303da5826a",
   "metadata": {},
   "source": [
    "### 实战：Temperatrue实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b43fad0b-0dd5-4698-a29d-eaca4817d3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== temperature = 0.0 ===\n",
      "大型语言模型（Large Language Models, LLMs）“天生不稳定”这一说法，实际上并非指它们在技术上无法稳定运行，而是指它们在**行为、输出和推理过程中的不可预测性与不一致性**，这种现象源于其设计原理和训练机制。以下是几个关键原因，解释为什么 LLM 被认为“天生不稳定”：\n",
      "\n",
      "---\n",
      "\n",
      "### 1. **基于概率的生成机制**\n",
      "LLM 的核心是**概率建模**：它们通过学习海量文本数据中词语之间的统计关联，预测下一个词的概率分布。这意味着：\n",
      "- 模型不会“知道”答案，而是“猜测”最可能的词序列。\n",
      "- 即使输入完全相同，由于采样策略（如 temperature、top-k、top-p）不同，输出也可能变化。\n",
      "- 这种随机性使得模型输出在某些情况下看似“不稳定”。\n",
      "\n",
      "👉 **举例**：问同一个问题两次，模型可能给出两个风格或内容略有差异的回答。\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **缺乏真正的理解与逻辑推理能力**\n",
      "LLM 不具备人类意义上的“理解”或“意识”。它们只是根据上下文和模式匹配生成文本，因此：\n",
      "- 对复杂、抽象或矛盾的问题，可能产生自相矛盾的回应。\n",
      "- 在逻辑推理任务中容易“自圆其说”，但结论可能是错的。\n",
      "- 面对未见过的场景时，容易“胡编乱造”（hallucination）。\n",
      "\n",
      "👉 这种“看似合理但实际错误”的输出，让人感觉模型“不可靠”或“不稳定”。\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **对输入微小变化敏感（幻觉与漂移）**\n",
      "即使输入仅有一个字的变化，模型的输出可能显著不同。例如：\n",
      "- “请告诉我如何做蛋糕” → 详细食谱\n",
      "- “请告诉我如何做‘蛋糕’” → 可能误解为“蛋糕”是一种宗教仪式\n",
      "\n",
      "这种对输入扰动的高度敏感，使得模型行为难以预测，表现为“不稳定”。\n",
      "\n",
      "---\n",
      "\n",
      "### 4. **训练数据的噪声与偏见**\n",
      "LLM 训练数据来自互联网，包含大量不一致、错误、争议或偏见信息。这导致：\n",
      "- 模型学到的“常识”可能是矛盾的。\n",
      "- 对同一主题可能有多个“合理”但互相冲突的版本。\n",
      "- 模型在面对模糊或边界情况时，选择哪个“合理”版本是不确定的。\n",
      "\n",
      "👉 结果就是：同样的问题，在不同时间或不同设置下，可能得到不同的回答。\n",
      "\n",
      "---\n",
      "\n",
      "### 5. **没有明确的目标函数或一致性约束**\n",
      "与传统程序不同，LLM 没有“正确答案”的硬性标准。它的目标是最大化生成文本的似然度，而不是保证事实准确或逻辑一致。\n",
      "- 因此，它倾向于生成“听起来合理”的内容，而非“事实正确”的内容。\n",
      "- 缺乏内在一致性检查机制，导致输出可能前后矛盾。\n",
      "\n",
      "---\n",
      "\n",
      "### 6. **后训练与提示工程的影响**\n",
      "模型在部署后常通过提示（prompt）引导，而提示的细微差异（如措辞、顺序、示例）可能导致截然不同的结果。这就放大了“不稳定性”：\n",
      "- 同一模型，用不同提示，可能表现迥异。\n",
      "- 模型对“提示”的敏感度高，进一步削弱其可预测性。\n",
      "\n",
      "---\n",
      "\n",
      "### 总结：为什么说“天生不稳定”？\n",
      "> **不是模型会崩溃，而是它根本不是一个确定性系统——它是概率性的、上下文依赖的、基于模式匹配而非逻辑推理的生成器。**\n",
      "\n",
      "因此，“不稳定”本质上是其**本质特性**，而非缺陷。它像一个天才但情绪多变的作家：总能写出精彩句子，但有时也会信口开河、前后矛盾、随心情改主意。\n",
      "\n",
      "---\n",
      "\n",
      "### 如何缓解这种“不稳定性”？\n",
      "虽然不能根除，但可通过以下方式降低风险：\n",
      "- 使用更严格的 prompt 工程（如 Chain-of-Thought）\n",
      "- 引入外部知识库（RAG）\n",
      "- 使用思维链（Chain of Thought）、自我一致性（Self-Consistency）等推理策略\n",
      "- 后处理校验（如事实核查、逻辑检查）\n",
      "- 集成多个模型投票或使用强化学习进行对齐（如 RLHF）\n",
      "\n",
      "---\n",
      "\n",
      "📌 **结论**：\n",
      "> LLM 天生不稳定，是因为它们不是“计算引擎”，而是“概率性语言生成器”。这种“不稳定”是其强大灵活性的代价，也是当前人工智能局限性的体现。理解这一点，有助于我们更理性地使用和评估大模型。\n",
      "\n",
      "=== temperature = 0.3 ===\n",
      "大型语言模型（LLM，Large Language Models）“天生不稳定”这一说法，并不完全准确，但确实反映了在实际应用中一些常见的挑战。我们可以从多个角度来理解为什么人们会觉得大模型“不稳定”，以及这种“不稳定性”的根源。\n",
      "\n",
      "---\n",
      "\n",
      "### 一、什么是“不稳定”？\n",
      "这里的“不稳定”通常指：\n",
      "- 输出结果不一致（相同输入可能得到不同回答）\n",
      "- 产生错误或幻觉（hallucination）——编造事实\n",
      "- 对微小输入变化反应剧烈（对扰动敏感）\n",
      "- 生成内容质量波动大\n",
      "- 在不同上下文或提示（prompt）下表现差异显著\n",
      "\n",
      "这些现象让人感觉模型“不可靠”或“难以预测”。\n",
      "\n",
      "---\n",
      "\n",
      "### 二、为什么会出现这种“不稳定性”？\n",
      "\n",
      "#### 1. **模型是概率性的，而非确定性的**\n",
      "- 大模型本质上是一个**基于概率的生成系统**。它通过预测下一个词的概率分布来生成文本。\n",
      "- 即使输入完全相同，采样过程（如 `top-k`、`temperature` 等）也会引入随机性。\n",
      "- 例如：使用高温度（temperature）时，输出更随机；低温度时更保守，但仍可能因采样差异导致不同结果。\n",
      "\n",
      "✅ 结论：**不是模型“出错”，而是其设计本就是随机的**。这并非缺陷，而是其表达多样性和创造力的基础。\n",
      "\n",
      "#### 2. **训练数据的复杂性与噪声**\n",
      "- LLM 训练数据来自互联网，包含大量不一致、矛盾甚至错误的信息。\n",
      "- 模型学习的是“统计规律”，而不是“真理”。因此它可能“合理地”生成一个听起来正确但事实上错误的内容（即幻觉）。\n",
      "- 例如：模型可能根据“某人出生于1980年”和“他活了150岁”等信息，推断出一个虚构的生平。\n",
      "\n",
      "✅ 结论：模型没有“真实感”，它只是模仿语言模式，无法验证事实真伪。\n",
      "\n",
      "#### 3. **上下文依赖性强，微小变化引发巨大差异**\n",
      "- 模型对输入提示（prompt）极其敏感。哪怕只改一个词，也可能导致输出完全不同。\n",
      "- 例如：\n",
      "  - “请解释量子力学” → 详细科普\n",
      "  - “请用小学生能懂的方式解释量子力学” → 完全不同的风格和深度\n",
      "\n",
      "✅ 原因：模型是“上下文感知”的，它的响应高度依赖于提示的措辞、语气、结构。\n",
      "\n",
      "#### 4. **缺乏真正的推理与常识**\n",
      "- 模型没有意识、没有世界知识库，它只是在“拟合”语言模式。\n",
      "- 当遇到超出训练数据范围的问题（如新概念、极端情况），模型容易“胡说八道”。\n",
      "- 它不会“思考”，而是“回忆”并重组已有语料。\n",
      "\n",
      "✅ 举例：问“如果猫会飞，它会怎么飞？”——模型可能给出看似合理的描述，但这是虚构逻辑。\n",
      "\n",
      "#### 5. **训练目标与人类期望存在偏差**\n",
      "- 模型的目标是“最大化下一个词的预测准确性”，而不是“生成真实、一致、安全的内容”。\n",
      "- 因此，它可能在“流畅度”上表现优秀，但在“真实性”、“一致性”、“安全性”上表现不佳。\n",
      "\n",
      "---\n",
      "\n",
      "### 三、是否“天生”就不稳定？\n",
      "\n",
      "⚠️ 关键点：**这不是“天生”的缺陷，而是“设计使然”**。\n",
      "\n",
      "- 如果我们要求模型“绝对稳定”、“永远正确”、“无幻觉”，那它就不再是语言模型，而是一个逻辑推理引擎或知识数据库。\n",
      "- 但现实需求是：我们需要一个能**灵活应对各种任务、生成自然语言、具备创造性的系统**。\n",
      "- 所以，某种程度的“不稳定性”是**为了换取灵活性、泛化能力和创造性**。\n",
      "\n",
      "---\n",
      "\n",
      "### 四、如何缓解“不稳定性”？\n",
      "\n",
      "虽然不能彻底消除，但可通过以下方式降低风险：\n",
      "\n",
      "| 方法 | 说明 |\n",
      "|------|------|\n",
      "| **提示工程（Prompt Engineering）** | 优化输入提示，明确指令，减少歧义 |\n",
      "| **温度控制 & 采样策略** | 降低 temperature，使用 beam search，提高输出一致性 |\n",
      "| **后处理与过滤** | 使用规则、关键词检查、事实核查工具 |\n",
      "| **检索增强生成（RAG）** | 引入外部知识源，减少幻觉 |\n",
      "| **模型微调（Fine-tuning）** | 针对特定任务调整模型行为 |\n",
      "| **多轮对话与上下文管理** | 保持一致性，避免前后矛盾 |\n",
      "\n",
      "---\n",
      "\n",
      "### ✅ 总结：为什么人们觉得 LLM “天生不稳定”？\n",
      "\n",
      "> **因为它们是概率生成模型，依赖统计规律而非逻辑推理，对输入敏感，且缺乏真实世界知识。它们的“不稳定性”其实是其灵活性与创造力的代价。**\n",
      "\n",
      "但这并不意味着它们“不好”或“不可用”。相反，只要理解其局限，并配合合适的工程手段，LLM 可以在许多场景中表现出色。\n",
      "\n",
      "---\n",
      "\n",
      "🔹 类比：  \n",
      "就像一个天才学生，记忆力超强、思维跳跃、善于联想，但有时会“信口开河”或“答非所问”。你不能指望他像机器一样精确，但你可以通过提问技巧让他发挥最大价值。\n",
      "\n",
      "---\n",
      "\n",
      "如果你希望我进一步解释某个方面（比如幻觉、提示工程、RAG等），欢迎继续提问！\n",
      "\n",
      "=== temperature = 0.7 ===\n",
      "大型语言模型（LLM，Large Language Models）“天生不稳定”这一说法并非指它们在物理或工程意义上的不稳定性，而是指它们在行为、输出和推理过程中存在一些固有的不可预测性与脆弱性。这种“不稳定性”源于其设计原理、训练方式以及对复杂语言和世界知识的建模方式。以下是几个关键原因，解释为什么 LLM 被认为“天生不稳定”：\n",
      "\n",
      "---\n",
      "\n",
      "### 1. **基于概率的生成机制**\n",
      "LLM 的核心是**自回归概率建模**：它们根据上下文预测下一个词的概率分布，并从中采样生成文本。\n",
      "\n",
      "- 这意味着每次生成都带有随机性（如温度参数控制），即使输入完全相同，输出也可能不同。\n",
      "- 模型并不“理解”语义，只是在统计上逼近人类语言模式。\n",
      "- 因此，相同的输入可能导致不同的输出结果——这就是行为上的“不一致”。\n",
      "\n",
      "> ✅ 例如：问“请解释量子力学”，两次生成可能一个简洁通俗，另一个冗长晦涩，甚至出现错误。\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **训练数据的多样性与噪声**\n",
      "LLM 训练依赖于互联网规模的数据，这些数据包含大量：\n",
      "- 不同观点、事实错误、偏见、矛盾信息；\n",
      "- 非标准语言、俚语、伪科学表达。\n",
      "\n",
      "由于模型试图“拟合”所有这些内容，它可能会：\n",
      "- 在面对模糊或矛盾问题时产生矛盾回答；\n",
      "- 学习到错误的因果关系或逻辑链条；\n",
      "- 对同一问题做出风格或立场截然不同的回应。\n",
      "\n",
      "> 🔍 举例：当被问“地球是不是平的？”，模型可能在某些情况下因训练数据中存在反智言论而“误信”或“含糊回应”。\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **幻觉（Hallucination）现象**\n",
      "这是最典型的“不稳定性”表现之一：模型会自信地生成**看似合理、但完全虚构的信息**。\n",
      "\n",
      "- 原因：模型不是从“知识库”检索答案，而是通过模式匹配生成连贯文本；\n",
      "- 它可以“编造”出不存在的引文、事件、人物、公式等；\n",
      "- 且无法自我校验真实性，常把“合理推测”当作“事实”。\n",
      "\n",
      "> ⚠️ 例如：“爱因斯坦曾说过‘时间是相对的’” —— 可能说对了，但若模型编造一句“他在1905年发表过一篇名为《光速悖论》的文章”，那就是幻觉。\n",
      "\n",
      "---\n",
      "\n",
      "### 4. **微小输入变化导致巨大输出差异（敏感性）**\n",
      "LLM 对输入极其敏感，哪怕改一个字，输出也可能天差地别。\n",
      "\n",
      "- 这是由于模型内部的非线性变换和高维空间中的“梯度爆炸”特性；\n",
      "- 输入的细微扰动（如标点、语气、措辞）可能引发完全不同路径的推理。\n",
      "\n",
      "> 📌 例子：\n",
      "> - “写一段关于猫的赞美诗” → 诗意优美；\n",
      "> - “写一段关于猫的赞美诗（讽刺语气）” → 变成挖苦讽刺。\n",
      "\n",
      "---\n",
      "\n",
      "### 5. **缺乏真正的因果理解与常识推理**\n",
      "虽然模型能模仿人类的语言逻辑，但它没有真实的世界模型或因果机制。\n",
      "\n",
      "- 它不能像人一样真正“思考”因果关系；\n",
      "- 当遇到超出训练范围的新情境时，容易陷入逻辑断裂或荒谬结论；\n",
      "- 表现为“聪明却肤浅”的不一致性。\n",
      "\n",
      "> 💡 举个经典案例：  \n",
      "> “如果所有的狗都会飞，那么猫会飞吗？”  \n",
      "> 模型可能按逻辑推演“是的”，但其实这个前提本身就是荒谬的——人知道这属于“虚假假设”，但模型可能认真回应。\n",
      "\n",
      "---\n",
      "\n",
      "### 6. **对提示（Prompt）高度依赖**\n",
      "模型的行为极大依赖于输入提示的设计，同一问题用不同方式提问，结果可能完全不同。\n",
      "\n",
      "- 提示工程（Prompt Engineering）成为“必修课”，说明模型本身不具备鲁棒的推理能力；\n",
      "- 没有标准指令，模型无法稳定地遵循意图。\n",
      "\n",
      "> ❗ 例如：“总结一下这个段落” 和 “请用三句话概括这段文字，每句不超过10个词” → 产出可能完全不同。\n",
      "\n",
      "---\n",
      "\n",
      "### 总结：为何说 LLM 天生不稳定？\n",
      "\n",
      "| 特征 | 说明 |\n",
      "|------|------|\n",
      "| ✅ 概率生成 | 输出非确定性，每次运行可能不同 |\n",
      "| ✅ 数据污染 | 学习了矛盾、错误、偏见的信息 |\n",
      "| ✅ 幻觉 | 编造事实，自信错误 |\n",
      "| ✅ 敏感输入 | 微小变化导致大输出波动 |\n",
      "| ✅ 缺乏真理解 | 无因果、无常识，只靠模式匹配 |\n",
      "| ✅ 提示依赖 | 行为受提示形式严重制约 |\n",
      "\n",
      "> 🧠 所以，“天生不稳定”本质上是：  \n",
      "> **一个基于统计规律而非逻辑真理的系统，在处理复杂、开放的人类语言时，必然表现出的不确定性与不可控性。**\n",
      "\n",
      "---\n",
      "\n",
      "### 如何缓解这种“不稳定性”？\n",
      "尽管“天生不稳定”，但可通过以下方法减轻影响：\n",
      "- 使用更精确的 prompt；\n",
      "- 引入外部知识检索（RAG）；\n",
      "- 启用思维链（Chain-of-Thought）、自我一致性检查；\n",
      "- 结合规则或专家系统做后处理；\n",
      "- 采用强化学习对齐（RLHF）优化输出质量。\n",
      "\n",
      "---\n",
      "\n",
      "### 小结金句：\n",
      "> “LLM 不是推理机器，而是语言模拟器。  \n",
      "> 它的‘不稳定’，正是它‘像人’却又‘不像人’的体现。”\n",
      "\n",
      "---\n",
      "\n",
      "如果你想了解如何让这类模型更稳定，我也可以提供具体技术策略。\n",
      "\n",
      "=== temperature = 1.0 ===\n",
      "“LLM 天生不稳定”这一说法实际上并不准确，但确实存在一种误解或部分事实的背景。我们可以从以下几个层面来澄清和解释为什么人们会认为“大语言模型（LLM）天生不稳定”，并说明真实情况：\n",
      "\n",
      "---\n",
      "\n",
      "### 一、误解的来源：为什么有人说 LLM “天生不稳定”？\n",
      "\n",
      "1. **输出不可控性**  \n",
      "   - 尽管训练数据庞大，但当输入非常规、模糊或带有误导性时，生成的内容可能看似随机或不一致。\n",
      "   - 例如，同一个问题两次提问，回答略有不同，这让人误以为模型“不稳定”。\n",
      "\n",
      "2. **对相同提示的不同响应**  \n",
      "   - 模型在推理阶段使用采样机制（如 temperature, top-k, top-p），这些参数会导致每次生成结果略有不同，即使输入完全相同。\n",
      "   - 这种“随机性”常被误认为是“不稳定”，而实际上这是可控的不确定性设计。\n",
      "\n",
      "3. **幻觉（Hallucination）现象**  \n",
      "   - 模型有时会“编造”信息，比如虚构不存在的事实或引用无效文献。\n",
      "   - 这看起来像是“错误输出”，但本质上是模型在缺乏确凿证据时的推测性生成，不是系统性崩溃。\n",
      "\n",
      "4. **上下文敏感与“遗忘”**  \n",
      "   - 在长文本中，模型可能会忽略早期信息或“忘记”对话历史，导致前后矛盾。\n",
      "   - 看似“不一致”，实则是其注意力机制限制和上下文窗口长度的物理瓶颈。\n",
      "\n",
      "5. **对微小输入变化反应剧烈**  \n",
      "   - 有时轻微改写问法就会导致答案完全不同，给人一种“敏感”甚至“紊乱”的印象。\n",
      "\n",
      "---\n",
      "\n",
      "### 二、真正原因：稳定性 ≠ 输出一致性\n",
      "\n",
      "我们需要区分“稳定性”的两个维度：\n",
      "\n",
      "| 维度 | 解释 | 为何被误视为“不稳定” |\n",
      "|------|------|------------------------|\n",
      "| 1. 输出确定性 | 同等输入是否产生同样输出 | 由于随机采样算法，同一输入常有不同输出 |\n",
      "| 2. 逻辑/事实一致性 | 生成内容是否符合常识或事实 | 幻觉、上下文失效等导致“不靠谱” |\n",
      "\n",
      "👉 实际上，**现代大型语言模型在训练完成后，其内在行为和权重分布是稳定的（即“神经网络收敛了”）**。它的“不稳定性”更多来源于：\n",
      "- 人为设定的采样策略（如高温度带来更随机输出）\n",
      "- 输入语义模糊性\n",
      "- 缺乏精确控制机制\n",
      "- 上下文记忆有限\n",
      "\n",
      "---\n",
      "\n",
      "### 三、如何理解“天生不稳定”这个说法？\n",
      "\n",
      "**“天生不稳定”这个说法，实际上是将‘不确定性’误读为‘系统不稳定’。**\n",
      "\n",
      "- ✅ 稳定性应该指：模型不会因轻微扰动而崩溃或出现极端异常。\n",
      "- ❌ 但用户感受到的“不稳”其实是：\n",
      "   - 基于概率的生成机制\n",
      "   - 多解性的自然体现（一个问题可以有多种合理回答）\n",
      "   - 对复杂语义理解的局限\n",
      "\n",
      "👉 换句话说：**不是模型“不稳定”，而是人类期望它像一个“确定性机器”一样工作——而这恰恰不符合其本质。**\n",
      "\n",
      "---\n",
      "\n",
      "### 四、结论：不是天生不稳定，而是“非确定性智能”\n",
      "\n",
      "- ✅ 正确理解应为：**大型语言模型是一种基于统计规律的生成式人工智能，本身具有不确定性和开放性，这是其强大泛化能力的代价。**\n",
      "- 🔄 所谓“不稳定”其实是“灵活多变、适应性强”的另一面。\n",
      "- 🔧 通过调整超参数（如temperature）、引入约束生成（如prompt engineering、RAG、强化学习）、增加事实校验模块（如外部知识库），我们完全可以显著提升输出的一致性和可靠性。\n",
      "\n",
      "---\n",
      "\n",
      "### 补充类比（帮助理解）：\n",
      "\n",
      "> 想象一个博学但喜欢自由发挥的人类学者：\n",
      "> - 他对同一问题有时答得完整，有时简洁；\n",
      "> - 他可能偶尔“加戏”说个不存在的典故；\n",
      "> - 允许你重复提问获得不同视角。\n",
      "\n",
      "这看似“不靠谱”，但实际上是其丰富知识与创造力的表现。大模型也是如此。\n",
      "\n",
      "---\n",
      "\n",
      "### 总结一句话：\n",
      "\n",
      "> **大语言模型并非“天生不稳定”，而是因其概率性、生成式本质，在面对模糊输入或需要创造性响应时表现出的“可变性”被误解为不稳定性。这种“不一致”恰恰是其灵活性和智能化的体现。**\n",
      "\n",
      "---\n",
      "\n",
      "如果你希望让模型更稳定，可以通过以下方式实现：\n",
      "- 减少采样随机性（降低 temperature）\n",
      "- 使用确定性解码（greedy decoding）\n",
      "- 引入 RAG（检索增强生成）\n",
      "- 加强事实一致性验证\n",
      "- 构建清晰的 prompt 框架\n",
      "\n",
      "这样就能把“看似不稳”的系统，变成可信赖的智能助手。\n"
     ]
    }
   ],
   "source": [
    "temperatures = [0.0, 0.3, 0.7, 1.0]\n",
    "\n",
    "prompt = \"请解释为什么 LLM 天生不稳定。\"\n",
    "\n",
    "for t in temperatures:\n",
    "    print(f\"\\n=== temperature = {t} ===\")\n",
    "    print(ask_llm(prompt, temperature=t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf0a0c6-8b68-46c5-8bc6-27348a69890e",
   "metadata": {},
   "source": [
    "你可以多运行几次对比下不同temperature下的返回表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15001e0-ebcb-4038-8280-d819900b6300",
   "metadata": {},
   "source": [
    "值得注意的是，由于由于 LLM 底层采样具有随机性，即使 Temperature 设置得再低，有时也难保两次输出完全一致。所以像阿里云大模型的API中加入了Seed (随机数种子)参数，如果你在两次请求中输入了相同的 Prompt、相同的 Temperature 以及相同的 Seed，那么模型大概率会给你生成完全相同的答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9acdbd-057a-4cbf-96b9-eeb2b4e55ecc",
   "metadata": {},
   "source": [
    "## 三、LLM 擅长什么，不擅长什么"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcd73cb-04b0-4c7e-ba79-38fa2e5ce166",
   "metadata": {},
   "source": [
    "基于LLM 的底层是概率分布和模式识别，这决定了它在处理**非结构化、有灵活度**的任务时表现惊人，但也有其短板。\n",
    "\n",
    "### LLM 擅长的：\n",
    "\n",
    "* **模式匹配**：识别文本中的结构、风格或隐含的逻辑。比如给模型一段你的写作风格，让它模仿。它能迅速捕捉你常用的句式、语气。\n",
    "* **语言生成*：将抽象的意图转化为流畅、多样的表达。比如要求“将这段技术文档改写成给 5 岁小孩听的故事”，它能完美完成语义的跨域平移。\n",
    "* **近似推理**：处理没有标准答案、需要常识和直觉的推理。比如判断一段对话中用户的潜在情绪。模型不是靠规则判断，而是靠“感受”词汇间的语义距离。\n",
    "* **模糊问题**：理解不完整、不精确的指令。比如你输入“那个讲外星人到地球结果发现人类很落后的老科幻电影叫什么？”（即使信息模糊，它也能匹配到《地球停转之日》）。\n",
    "\n",
    "### LLM 不擅长的：\n",
    "\n",
    "LLM 的致命弱点源于它**“不查表、不计算”**。它在面对需要绝对精确性的任务（比如逻辑与事实）时极易翻车。\n",
    "\n",
    "* **精确计算**：模型是在预测数字的“长相”，而不是在执行算术逻辑。比如询问计算结果时，它可能会给出一个“看起来很像正确答案”的数字，但由于进位错误，末尾几位往往是错的。\n",
    "* **事实校验 **：它优先保证句子的“通顺”，而非“真实”。这就是所谓的“幻觉”。比如询问“2023 年诺贝尔文学奖获得者写的关于赛博朋克的小说叫什么？”即使该作者没写过，模型也可能编造出一个书名。\n",
    "* **长期一致性**：受限于上下文窗口，生成内容过长时，模型会忘记前文的设定。比如在写长篇小说时，第 1 章设定主角是左撇子，第 50 章可能就变成了右手。\n",
    "* **边界条件 (Edge Cases)***：对于罕见、极端的逻辑问题，由于训练数据覆盖不足，模型常表现出“过度自信的错误”。\n",
    "* **自我纠错 (Self-Correction)**：模型往往会顺着自己的错话继续往下编，很难在生成过程中突然意识到“我刚才说错了”。比如当模型算错题时，如果你问“你确定吗？”，它常会道歉并给出**另一个**同样错误的答案，而不是重新审视逻辑。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a88b386-24ba-457c-a54b-abf9cc02a2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
